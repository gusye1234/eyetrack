## Explain the eye-tracking model

[TOC]
### introduction

(Chinese)

æˆ‘ä»¬å¸Œæœ›å¯ä»¥æ‰¾åˆ°ä¸€ç§ç†è§£ç¥ç»ç½‘ç»œçš„æ–¹æ³•, å¯ä»¥ç†è§£åœ¨ç½‘ç»œä¸­é‡è¦çš„ç‰¹å¾æœ‰å“ªäº›, ä»¥åŠåœ¨å°½å¯èƒ½ä¿è¯æ¨¡å‹è¡¨ç°çš„æƒ…å†µä¸‹, æˆ‘ä»¬æ ¹æ®æ­¤è§£é‡Šæ€§å¯¹æ¨¡å‹å¯ä»¥è¿›è¡Œæ€æ ·çš„ä¼˜åŒ–

å¦‚ä»Šçš„å·ç§¯ç¥ç»ç½‘ç»œä¸­å¤§å¤šç»“æ„ä¸º

1. è®¾è®¡ç‰¹å¾æå–å™¨, ä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œ$Conv Net$æ¥è·å– feature map $f$
2. å°† $f$  å‘é‡åŒ–å¾—åˆ°$embedding_{f}$ , è¾“å…¥åˆ°å…¨è¿æ¥å±‚$FCNet$ä¸­è¿›è¡Œå›å½’æˆ–è€…é¢„æµ‹

åœ¨ä¸€ä¸ªå·²ç»è®­ç»ƒå¥½çš„æ¨¡å‹ä¸­, ç”±$f$ å¾—æ¥çš„$embedding_{f}$ å¯ä»¥ä½œä¸ºè¾“å…¥å›¾ç‰‡çš„ ä½ç»´è¡¨å¾(embedding), æˆ–è€…è¯´, æ˜¯å›¾ç‰‡ä¿¡æ¯ä¸­ä¸ç›®æ ‡è¾“å‡ºç›¸å…³çš„ ä½ç»´è¡¨å¾. å¦‚åœ¨çœ¼çƒè·Ÿè¸ªä¸­, $embedding_f$ ä¸­çš„ä¿¡æ¯åº”è¯¥ä¸å‡è§†ç‚¹æœ‰å…³.

ä½†æ˜¯ç½‘ç»œä¸­æå–å‡ºæ¥çš„$embedding_f$ ä¸­éƒ½æ˜¯æœ‰ç”¨çš„ä¿¡æ¯å—?  å¾€å¾€ç¥ç»ç½‘ç»œæ¶æ„å¸ˆä»…ä»…è¢«æ¨¡å‹åœ¨é¢„æµ‹ä¸Šçš„è¡¨ç°ä¸»å¯¼, å¾€å¾€å¯¹æ¨¡å‹æœ¬èº«çš„å®¹é‡åˆ©ç”¨ç¨‹åº¦(åè¯å¾…å®š)æ²¡æœ‰æ˜ç¡®çš„è®¤è¯†,

> Clearly, when it is possible to mimic the function learned by a complex model with a small net, the function learned by the complex model wasnâ€™t truly too complex to be learned by a small net. This suggests to us that the complexity of a learned model, and the size and architecture of the representation best used to learn that model, are different things... [5]

æˆ‘ä»¬å¸Œæœ›å¯ä»¥ç›´è§‚çš„è§£é‡Š, ç½‘ç»œä¸­å“ªäº›è®¡ç®—è·¯å¾„æ˜¯çœŸæ­£å½±å“ç»“æœçš„, è€Œå¦å¤–çš„ä¸€äº›åˆ™å¯¹æœ€åçš„è¾“å‡ºå½±å“ä¸å¤§, è¿™äº›ä¿¡æ¯å¯ä»¥æŒ‡å¯¼ç ”ç©¶äººå‘˜æ„é€ ä¸€ä¸ªå®¹é‡æ›´åˆç†æ›´å°çš„ç½‘ç»œæ¥è¾¾åˆ°ç›¸ä¼¼çš„ç›®æ ‡.

ä½†æ˜¯, ä»…ä»…ç¼©å°ç½‘ç»œè¿›è¡Œè®­ç»ƒå¾€å¾€å¾—ä¸åˆ°ä¸å®¹é‡æ›´å¤§çš„æ¨¡å‹ç›¸è¿‘çš„ç²¾åº¦, å¾€å¾€éœ€è¦ä¸æ¨¡å‹è’¸é¦çš„æ–¹æ³•ç»“åˆ. 

> Surprisingly, often it is not (yet) possible to train a small neural net on the original training data to be as accurate as the complex model, nor as accurate as the mimic model. Compression demonstrates that a small neural net could, in principle, learn the more accurate function, but current learning algorithms are unable to train a model with that accuracy from the original training data; instead, we must train the complex intermediate model first and then train the neural net to mimic it...[5]

è€Œç°åœ¨çš„æ¨¡å‹è’¸é¦æˆ–å‹ç¼©å¾€å¾€é›†ä¸­äºå·ç§¯å±‚çš„ä¼˜åŒ–, è€Œå¯¹äºå…¨è¿æ¥å±‚çš„ä¼˜åŒ–è¾ƒå°‘. ä½†äº‹å®ä¸Š, é™¤äº†ææ·±çš„ä¸€äº›ç½‘ç»œ(å¦‚ ResNet, VGG)å¤–, ä¸€ä¸ª 5-10 å±‚çš„å·ç§¯ç½‘ç»œä¸­, å¾€å¾€å·ç§¯å±‚å‚æ•°æ•°é‡ä¸å¯¹åº”çš„å…¨è¿æ¥åˆ†ç±»å±‚çš„å‚æ•°æ•°é‡éƒ½æ˜¯ååˆ†å¤§çš„. å¦‚æœ¬æ¬¡å®éªŒæ‰€ä½¿ç”¨çš„æ¨¡å‹

- ä¸€ä¸ªå·ç§¯ç½‘ç»œå¯¹åº”çš„æ‰€æœ‰å‚æ•°ä¸º 85560, æ¨¡å‹ä¸­æ‰€ä½¿ç”¨çš„å››ä¸ªç½‘ç»œå‚æ•°æ€»é‡ä¸º 342240
- è€Œå¯¹åº”çš„å…¨è¿æ¥å±‚, ä»…ä»…æ˜¯ç¬¬ä¸€å±‚, å°†æ‘Šå¹³(flatten)çš„ç‰¹å¾å›¾è½¬åŒ–æˆ 128 ç»´å‘é‡, å°±æœ‰ 1089536 å‚æ•°
- ä¸Šè¿°è¿™äº›çš†æ˜¯å¯è®­ç»ƒçš„å‚æ•°, å¦‚æœä»…ä»å‚æ•°é‡æ¥è¯´, å…¨è¿æ¥å±‚è¿™ä¸€å—æ˜¯å€¼å¾—å‹ç¼©çš„



### developing environment

* macOS Mojave version 10.14
* Python 3.6.3
* pytorch 1.0.0 or higher
* torchvision0.4.0a0+6b959ee




### model

*  Refer to part of iTracker feaure extractor from [CSALI MIT](https://github.com/CSAILVision/GazeCapture)



### install

* `pip install requirements.txt`
* **prepare data**

under `root` 

```bash
mkdir data && cd data
mkdir train && mkdir test
```

copy data following the paths

```bash
data
â”œâ”€â”€ val
|		â”œâ”€â”€ 17
|   â”œâ”€â”€ 20
|   â”œâ”€â”€ etc
â”œâ”€â”€ test
â”‚Â Â  â”œâ”€â”€ 3
â”‚Â Â  â”œâ”€â”€ 7
â”‚Â Â  â””â”€â”€ etc
â””â”€â”€ train
    â”œâ”€â”€ 8
    â”œâ”€â”€ 9
    â””â”€â”€ etc
```

* `cd code && python main.py`

* follow the error traceback and see what's going on ğŸ˜

* There are some launch command examples:

  * `python main.py --doload=no --tensorboard=yes --lr=5e-4 --sigmoid=yes --opt=adam --delta=0.005 --tag="L2_adam_0.0005"`

  * `python main.py --doload=yes --eval=yes --weights="./checkpoints/best_L2_adam_0.0005checkpoint_sigmoid.pth-2.tar" --evalFolder=22 --sigmoid=yes`







### TODO

* pytorch's data format problems--solved

```python
# File "main.py"
m = m.float()
# File "utils.py", Function 'train' and 'validate'
output = model(data["img0"].float(), data["img1"].float(), data["img2"].float(), data["img3"].float())
```

maybe there are some more elegant way to do so.

- running on GPU platform--solved 
- HOW TO EXPLAIN?



### log

* 2019.8.22: complete `load_data.py` , return  `dict` type data. 

  including keys:

  * `img0`(left eye 0),`img1`(left eye 1), `im2`(right eye 3), `img3`(right eye 4) , image data format is `CHW`(channel first) ,shape`(-1, 1, 576, 720)`, type `torch.Tensor`
  * `label` , gaze point in image coordinate , type `torch.Tensor`
*  2019.8.23: add some image normalize tools  by [dearmrlv](https://github.com/dearmrlv)

* 2019.8.24: complete pipeline, and run it on CPU device

![program](./imgs/program.png)

* 2019.8.25: successfully running on GPU and add more options

```shell
eye-tracker-model.

optional arguments:
  -h, --help            show this help message and exit
  --tensorboard TENSORBOARD
                        ask if store the output to tensorboard
  --comment COMMENT
  --batch_size BATCH_SIZE
  --doload DOLOAD       load previous weights or not
  --weights WEIGHTS     weight file location
  --epochs EPOCHS       traing total epochs
  --lr LR               base learning rate
  --opt OPT             choose optimizer in [adam, SGD]
  --sigmoid SIGMOID     use simoid activation function in the last layer or
                        not
  --delta DELTA         Tolerance for early stoping
  --tag TAG             suffix of the weight file
  --eval EVAL           start eval mode
  --evalFolder EVALFOLDER
                        choose a test folder to generate prediction
  --resize RESIZE       resize picture to 256X256 (original 576X720)
  --generating GENERATING
                        generating the gradient heat map over the origin image
  --collect COLLECT     collect the intermediate embedding to middle.npy
  --activation ACTIVATION
                        choose the activation function to generate
                        intermediate embedding
```

* 2019.8.30: Trained a bunch of models, for now, best predicting error on test set is about **10 pixels**, still working on itğŸ¤¯

![demo](./imgs/demo.png)

(green for prediction, black for ground truth. use demo.py to generate video)

* add vis tools and experiment model to analyze

<img src="./imgs/result.png" alt="100% " style="zoom:100%;" />





